{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4pctsX9q8fYV8wfqgkeAp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimarta-devi/Safety_Helmet_Detection_YOLOv8_AS-One/blob/main/Copy_of_yolov8_custom_dataset_detect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzNEVYdF5cB3",
        "outputId": "81c3dad9-8d5a-4cb8-cbb5-a20b70886709"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install ultralytics==8.0.20\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ6qSYyM5jSe",
        "outputId": "3784d845-b7c5-41c4-9835-5438836b13a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.8.10 torch-1.13.1+cu116 CPU\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 24.4/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image\n",
        "from roboflow import Roboflow\n",
        "\n",
        "rf = Roboflow(api_key=\"Your API Key\")\n",
        "project = rf.workspace().project(\"hard-hat-sample-v3610\")\n",
        "dataset = project.version(2).download(\"yolov8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuHrVoCH6WTY",
        "outputId": "ecdc3bb7-c988-468d-c3b0-67c7087314b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.8/dist-packages (0.2.29)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.8/dist-packages (from roboflow) (4.6.0.66)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from roboflow) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.25.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.8/dist-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from roboflow) (6.0)\n",
            "Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.8/dist-packages (from roboflow) (2022.12.7)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.8/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from roboflow) (7.1.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.8/dist-packages (from roboflow) (0.21.1)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.8/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.26.14)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.21.6)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.8/dist-packages (from roboflow) (0.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from roboflow) (1.15.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.8/dist-packages (from roboflow) (2.4.7)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in Hard-Hat-Sample-2 to yolov8: 100% [6385715 / 6385715] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to Hard-Hat-Sample-2 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 491/491 [00:00<00:00, 737.42it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "\n",
        "!yolo task=detect mode=train model=yolov8n.pt data=/content/datasets/Hard-Hat-Sample-2/data.yaml epochs=10 imgsz=600 batch=4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWAslpH7CObt",
        "outputId": "8eb0fbb5-a7d6-47b2-c7b8-b2f1d070eae7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.8.10 torch-1.13.1+cu116 CPU\n",
            "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=/content/datasets/Hard-Hat-Sample-2/data.yaml, epochs=10, patience=50, batch=4, imgsz=600, save=True, cache=False, device=, workers=8, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, image_weights=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, overlap_mask=True, mask_ratio=4, dropout=False, val=True, save_json=False, save_hybrid=False, conf=0.001, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=ultralytics/assets/, show=False, save_txt=False, save_conf=False, save_crop=False, hide_labels=False, hide_conf=False, vid_stride=1, line_thickness=3, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, fl_gamma=0.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, save_dir=runs/detect/train2\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 16.2MB/s]\n",
            "Overriding model.yaml nc=80 with nc=3\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.Conv                  [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.Conv                  [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.C2f                   [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.Conv                  [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.C2f                   [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.Conv                  [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.C2f                   [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.Conv                  [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.C2f                   [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.SPPF                  [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.C2f                   [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.C2f                   [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.Conv                  [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.C2f                   [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.Conv                  [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.Concat                [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.C2f                   [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.Detect                [3, [64, 128, 256]]           \n",
            "Model summary: 225 layers, 3011433 parameters, 3011417 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "2023-02-14 15:12:27.171142: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-14 15:12:30.002797: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-14 15:12:30.002962: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/lib/python3.8/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-14 15:12:30.002984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "WARNING âš ï¸ --img-size [600] must be multiple of max stride 32, updating to [608]\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.001), 63 bias\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/Hard-Hat-Sample-2/train/labels... 210 images, 0 backgrounds, 0 corrupt: 100% 210/210 [00:00<00:00, 1707.33it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/Hard-Hat-Sample-2/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/Hard-Hat-Sample-2/valid/labels... 20 images, 0 backgrounds, 0 corrupt: 100% 20/20 [00:00<00:00, 2392.92it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/Hard-Hat-Sample-2/valid/labels.cache\n",
            "Image sizes 608 train, 608 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/10         0G      2.087      4.063      1.857          9        608: 100% 53/53 [02:35<00:00,  2.94s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:04<00:00,  1.66s/it]\n",
            "                   all         20         65    0.00246      0.478     0.0394     0.0204\n",
            "                  head         20         18    0.00194      0.167    0.00167   0.000369\n",
            "                helmet         20         45    0.00417      0.267     0.0299     0.0122\n",
            "                person         20          2    0.00127          1     0.0865     0.0485\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/10         0G      1.621       2.94      1.521          9        608: 100% 53/53 [02:29<00:00,  2.82s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:06<00:00,  2.05s/it]\n",
            "                   all         20         65      0.575      0.315       0.32      0.114\n",
            "                  head         20         18      0.232      0.278      0.365       0.15\n",
            "                helmet         20         45      0.492      0.667      0.568      0.182\n",
            "                person         20          2          1          0     0.0267    0.00802\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/10         0G      1.598      2.394      1.466          3        608: 100% 53/53 [02:30<00:00,  2.84s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:04<00:00,  1.62s/it]\n",
            "                   all         20         65      0.531      0.426      0.327      0.137\n",
            "                  head         20         18        0.2      0.389      0.169     0.0585\n",
            "                helmet         20         45      0.393      0.889       0.79      0.347\n",
            "                person         20          2          1          0     0.0214      0.006\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/10         0G      1.577      2.232      1.418         11        608: 100% 53/53 [02:32<00:00,  2.88s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:04<00:00,  1.63s/it]\n",
            "                   all         20         65      0.618      0.333      0.369      0.214\n",
            "                  head         20         18       0.26      0.111      0.259      0.145\n",
            "                helmet         20         45      0.594      0.889      0.836      0.493\n",
            "                person         20          2          1          0      0.013    0.00508\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/10         0G      1.519       2.08      1.364         13        608: 100% 53/53 [02:30<00:00,  2.83s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:05<00:00,  1.98s/it]\n",
            "                   all         20         65      0.677      0.367      0.353      0.122\n",
            "                  head         20         18      0.517      0.389      0.396      0.134\n",
            "                helmet         20         45      0.516      0.711      0.643      0.227\n",
            "                person         20          2          1          0     0.0195    0.00382\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/10         0G      1.503      1.949      1.333          7        608: 100% 53/53 [02:32<00:00,  2.87s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:06<00:00,  2.03s/it]\n",
            "                   all         20         65      0.795      0.394      0.437      0.203\n",
            "                  head         20         18      0.717      0.333      0.452      0.211\n",
            "                helmet         20         45      0.668       0.85      0.851      0.394\n",
            "                person         20          2          1          0    0.00849    0.00343\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/10         0G      1.502      1.908      1.351         11        608: 100% 53/53 [02:31<00:00,  2.85s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:04<00:00,  1.61s/it]\n",
            "                   all         20         65      0.791      0.407      0.459      0.245\n",
            "                  head         20         18      0.809      0.333       0.54      0.306\n",
            "                helmet         20         45      0.565      0.889      0.823      0.427\n",
            "                person         20          2          1          0     0.0139    0.00388\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/10         0G      1.419      1.889      1.296         10        608: 100% 53/53 [02:30<00:00,  2.84s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:04<00:00,  1.57s/it]\n",
            "                   all         20         65      0.456      0.449      0.529      0.237\n",
            "                  head         20         18      0.709      0.407      0.677       0.32\n",
            "                helmet         20         45      0.658       0.94      0.894      0.384\n",
            "                person         20          2          0          0      0.017    0.00499\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/10         0G      1.408      1.771      1.286          4        608: 100% 53/53 [02:30<00:00,  2.83s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:05<00:00,  1.72s/it]\n",
            "                   all         20         65      0.937      0.483      0.551      0.282\n",
            "                  head         20         18      0.885      0.611      0.736       0.39\n",
            "                helmet         20         45      0.926      0.839      0.902       0.45\n",
            "                person         20          2          1          0     0.0156    0.00551\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/10         0G      1.369      1.701      1.261         10        608: 100% 53/53 [02:30<00:00,  2.84s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:05<00:00,  1.90s/it]\n",
            "                   all         20         65      0.541      0.474      0.535      0.295\n",
            "                  head         20         18      0.714      0.556      0.666      0.406\n",
            "                helmet         20         45      0.909      0.867      0.929      0.474\n",
            "                person         20          2          0          0     0.0111    0.00446\n",
            "\n",
            "10 epochs completed in 0.437 hours.\n",
            "Optimizer stripped from runs/detect/train2/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train2/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train2/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.8.10 torch-1.13.1+cu116 CPU\n",
            "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 3/3 [00:05<00:00,  1.70s/it]\n",
            "                   all         20         65      0.586      0.467      0.538      0.295\n",
            "                  head         20         18       0.83      0.544      0.676      0.408\n",
            "                helmet         20         45      0.928      0.857      0.928      0.473\n",
            "                person         20          2          0          0     0.0111    0.00443\n",
            "Speed: 2.7ms pre-process, 226.8ms inference, 0.0ms loss, 10.1ms post-process per image\n",
            "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "\n",
        "!yolo task=detect mode=val model={HOME}/runs/detect/train2/weights/best.pt data=/content/datasets/Hard-Hat-Sample-2/data.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1eiuETSCmj3",
        "outputId": "16555d2f-0523-4191-b021-f99181ea9eed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.8.10 torch-1.13.1+cu116 CPU\n",
            "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/Hard-Hat-Sample-2/valid/labels.cache... 20 images, 0 backgrounds, 0 corrupt: 100% 20/20 [00:00<?, ?it/s]\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 2/2 [00:12<00:00,  6.47s/it]\n",
            "                   all         20         65      0.586      0.467      0.538      0.295\n",
            "                  head         20         18       0.83      0.544      0.676      0.408\n",
            "                helmet         20         45      0.928      0.857      0.928      0.473\n",
            "                person         20          2          0          0     0.0111    0.00443\n",
            "Speed: 12.2ms pre-process, 581.0ms inference, 0.0ms loss, 29.4ms post-process per image\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!yolo task=detect mode=predict model={HOME}/runs/detect/train2/weights/best.pt conf=0.25 source=/content/datasets/Hard-Hat-Sample-2/test/images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8XZM6vJJikl",
        "outputId": "6cffa3cc-098f-4b16-eb2e-94a01fd80d1b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Ultralytics YOLOv8.0.20 ðŸš€ Python-3.8.10 torch-1.13.1+cu116 CPU\n",
            "Model summary (fused): 168 layers, 3006233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "image 1/10 /content/datasets/Hard-Hat-Sample-2/test/images/000008_jpg.rf.ZKzEtaDzutx3g1tkRKli.jpg: 608x608 5 helmets, 326.9ms\n",
            "image 2/10 /content/datasets/Hard-Hat-Sample-2/test/images/000011_jpg.rf.5NFLWCeWL8xyYUbYPKVP.jpg: 608x608 3 heads, 15 helmets, 328.6ms\n",
            "image 3/10 /content/datasets/Hard-Hat-Sample-2/test/images/000034_jpg.rf.xrKwRgHNN19wDlTapUCi.jpg: 608x608 1 helmet, 319.1ms\n",
            "image 4/10 /content/datasets/Hard-Hat-Sample-2/test/images/000047_jpg.rf.t4BcXDebuSzAAEI9nrhD.jpg: 608x608 4 helmets, 322.9ms\n",
            "image 5/10 /content/datasets/Hard-Hat-Sample-2/test/images/000054_jpg.rf.DNzwW6khcBJNAjzCpnkr.jpg: 608x608 1 helmet, 318.0ms\n",
            "image 6/10 /content/datasets/Hard-Hat-Sample-2/test/images/000073_jpg.rf.BVwyvboC6R983EGomLnk.jpg: 608x608 7 helmets, 323.8ms\n",
            "image 7/10 /content/datasets/Hard-Hat-Sample-2/test/images/000076_jpg.rf.SNT8dWFNzjKaLHjlx9Hj.jpg: 608x608 1 helmet, 316.8ms\n",
            "image 8/10 /content/datasets/Hard-Hat-Sample-2/test/images/000084_jpg.rf.DMxD5Hkx62KbzG68JPH4.jpg: 608x608 8 helmets, 322.1ms\n",
            "image 9/10 /content/datasets/Hard-Hat-Sample-2/test/images/000097_jpg.rf.lIIcUISxPjEh9XG2OC5A.jpg: 608x608 1 helmet, 314.2ms\n",
            "image 10/10 /content/datasets/Hard-Hat-Sample-2/test/images/000098_jpg.rf.OcANJfC1TGE3RP8BT9ST.jpg: 608x608 4 heads, 3 helmets, 305.7ms\n",
            "Speed: 1.6ms pre-process, 319.8ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 608)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lsOdtrcshJBL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}